{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import mmsdk package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import SDK_PATH, DATA_PATH, WORD_EMB_PATH, CACHE_PATH\n",
    "import sys\n",
    "\n",
    "if SDK_PATH is None:\n",
    "    print(\"SDK path is not specified! Please specify first in constants/paths.py\")\n",
    "    exit(0)\n",
    "else:\n",
    "    sys.path.append(SDK_PATH)\n",
    "\n",
    "import mmsdk\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from mmsdk import mmdatasdk as md\n",
    "from subprocess import check_call, CalledProcessError\n",
    "\n",
    "# create folders for storing the data\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    check_call(' '.join(['mkdir', '-p', DATA_PATH]), shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparation import to modify mmsdk dataset alignment method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmsdk.mmdatasdk import log, computational_sequence\n",
    "import numpy\n",
    "import time\n",
    "import struct\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inheritate from mmsdk with new methods: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_htk_format(features, feature_type, folder, video_number):\n",
    "    '''\n",
    "    This function works for function align_upsampling_and_save. It save feature vectors of one video id into one \n",
    "    htk format file. \n",
    "    feature: feature of one video\n",
    "    All files are of USER type.\n",
    "    number of samples being consistent with number of samples in csd file\n",
    "    sample bytes for COVAREP: 4 * 74 = 296\n",
    "    sample period: 10 000.0 us (100000)\n",
    "    paramkind: USER (9)\n",
    "    '''\n",
    "    #set default extension\n",
    "    ext = '.txt'\n",
    "    file_name = os.path.join(folder, feature_type, video_number)\n",
    "\n",
    "    num_sample = len(features)\n",
    "    byte_n_sample = num_sample.to_bytes(4, byteorder='big')\n",
    "\n",
    "    period = 100000\n",
    "    byte_period = period.to_bytes(4, byteorder = 'big')\n",
    "\n",
    "    if feature_type == 'COVAREP':\n",
    "        sample_b = 296\n",
    "        ext = '.cov'\n",
    "    elif feature_type == 'WordVec':\n",
    "        sample_b = 1200\n",
    "        ext = '.wvec'\n",
    "\n",
    "    byte_sample_b = sample_b.to_bytes(2, byteorder = 'big')\n",
    "\n",
    "    sample_type = 9\n",
    "    byte_sample_type = sample_type.to_bytes(2, byteorder = 'big')\n",
    "    header = byte_n_sample + byte_period + byte_sample_b + byte_sample_type\n",
    "\n",
    "    output_byte = b''\n",
    "    '''\n",
    "    try:\n",
    "        print(type(features))\n",
    "        print(type(features[0]))\n",
    "    except:\n",
    "        print('features cannot be indexed')\n",
    "        \n",
    "    try:\n",
    "        print(features[0].shape)\n",
    "    except:\n",
    "        print('features[0] is not np array')\n",
    "    '''\n",
    "    for datapoint in features:\n",
    "        y = list(map(lambda x: struct.pack('>f', x), datapoint))\n",
    "        byte_datapoint = b''.join(y)\n",
    "\n",
    "        output_byte += byte_datapoint\n",
    "\n",
    "    with open(file_name + ext, 'wb') as file:\n",
    "        file.write(header + output_byte)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle, os, json, codecs\n",
    "\n",
    "\n",
    "def save_intervals(intervals, feature_type, folder, video_number):\n",
    "    #set default extension\n",
    "    ext = '.json'\n",
    "    video_number = 'intervals_' + video_number\n",
    "    file_name = os.path.join(folder, feature_type, video_number)\n",
    "    \n",
    "    if type(intervals) != list:\n",
    "        intervals = intervals.tolist()\n",
    "    json.dump(intervals, codecs.open(file_name + ext, 'w', encoding='utf-8'), indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class new_mmdataset(md.mmdataset):\n",
    "    #TODO: Need tqdm bar for this as well\n",
    "    def get_relevant_entries(self,reference):\n",
    "        relevant_entries={}\n",
    "        relevant_entries_np={}\n",
    "\n",
    "        #pbar = tqdm(total=count,unit=\" Computational Sequence Entries\",leave=False)\n",
    "\n",
    "        #otherseq_key: OpenFace, wordvec, etc\n",
    "        for otherseq_key in set(list(self.computational_sequences.keys()))-set([reference]):\n",
    "            relevant_entries[otherseq_key]={}\n",
    "            relevant_entries_np[otherseq_key]={}\n",
    "            sub_compseq=self.computational_sequences[otherseq_key]\n",
    "            # for some_id in all video ids\n",
    "            for key in list(sub_compseq.data.keys()):\n",
    "                keystripped=key.split('[')[0]\n",
    "                if keystripped not in relevant_entries[otherseq_key]:                           \n",
    "                    relevant_entries[otherseq_key][keystripped]={}\n",
    "                    relevant_entries[otherseq_key][keystripped][\"intervals\"]=[]                     \n",
    "                    relevant_entries[otherseq_key][keystripped][\"features\"]=[]                                                            \n",
    "                    \n",
    "                relev_intervals=self.computational_sequences[otherseq_key].data[key][\"intervals\"]                                             \n",
    "                relev_features=self.computational_sequences[otherseq_key].data[key][\"features\"]         \n",
    "                if len(relev_intervals.shape)<2:\n",
    "                    relev_intervals=relev_intervals[None,:]\n",
    "                    relev_features=relev_features[None,:]\n",
    "\n",
    "                relevant_entries[otherseq_key][keystripped][\"intervals\"].append(relev_intervals)\n",
    "                relevant_entries[otherseq_key][keystripped][\"features\"].append(relev_features)\n",
    "                \n",
    "            for key in list(relevant_entries[otherseq_key].keys()):\n",
    "                relev_intervals_np=numpy.concatenate(relevant_entries[otherseq_key][key][\"intervals\"],axis=0)                                 \n",
    "                relev_features_np=numpy.concatenate(relevant_entries[otherseq_key][key][\"features\"],axis=0)\n",
    "                sorted_indices=sorted(range(relev_intervals_np.shape[0]),key=lambda x: relev_intervals_np[x,0])                               \n",
    "                relev_intervals_np=relev_intervals_np[sorted_indices,:]                         \n",
    "                relev_features_np=relev_features_np[sorted_indices,:]\n",
    "\n",
    "                relevant_entries_np[otherseq_key][key]={}\n",
    "                relevant_entries_np[otherseq_key][key][\"intervals\"]=relev_intervals_np\n",
    "                relevant_entries_np[otherseq_key][key][\"features\"]=relev_features_np\n",
    "            log.status(\"Pre-alignment done for <%s> ...\"%otherseq_key)\n",
    "        return relevant_entries_np\n",
    "    \n",
    "    def intersect_and_copy_upsampling(self, ref_all, relevant_entry, epsilon, log_file, feature_info):\n",
    "        #ref_all: reference_entries[other_key][entry_key]['intervals'] e.g. [COVAREP][some video id]['intervals']\n",
    "        #relevant_entry: relevant_entries[other_key][entry_key] e.g. [COVAREP][some video id]\n",
    "        #epsilon: error allowed in alignment\n",
    "        #ref_time < one interval in relevant_entry\n",
    "        \n",
    "        pbar_small=log.progress_bar(total=ref_all.shape[0],unit=\" Segments\",leave=False)\n",
    "        pbar_small.set_description(\"Aligning: \" + feature_info)\n",
    "        \n",
    "        sub=relevant_entry[\"intervals\"]\n",
    "        features=relevant_entry[\"features\"]\n",
    "        \n",
    "        #finding where intersect happens\n",
    "        pointer_b = 0 # for relevant_entry\n",
    "        aligned_sub = []\n",
    "        aligned_feature = []\n",
    "        \n",
    "        for i, inter in enumerate(ref_all):\n",
    "            #print(pointer_b)\n",
    "            if (abs(inter[0]-inter[1])<epsilon):\n",
    "                pbar_small.update(1)\n",
    "                continue\n",
    "            pointer_c = pointer_b\n",
    "            while(pointer_c < sub.shape[0]):\n",
    "                if (inter[0] - sub[pointer_c][0]) > (-epsilon) and (sub[pointer_c][1] - inter[0]) > (-epsilon):\n",
    "                    aligned_sub.append(sub[pointer_c])\n",
    "                    aligned_feature.append(features[pointer_c])\n",
    "                    break\n",
    "                else:\n",
    "                    pointer_c += 1\n",
    "                \n",
    "            if pointer_c == sub.shape[0]:\n",
    "                diff = list(map(lambda x: abs(inter[0] - x), sub[:, 0]))\n",
    "                min_diff = min(diff)\n",
    "                pointer_c = diff.index(min_diff)\n",
    "                with open(log_file, 'w+') as fi:\n",
    "                    fi.write('no corresponding frame, find the closest one, {}, difference: {}\\n'.format(feature_info, min_diff))\n",
    "                aligned_sub.append(sub[pointer_c])\n",
    "                aligned_feature.append(features[pointer_c])\n",
    "            else:\n",
    "                pointer_b = pointer_c\n",
    "\n",
    "            pbar_small.update(1)\n",
    "        \n",
    "        aligned_sub = np.array(aligned_sub)\n",
    "        aligned_feature = np.array(aligned_feature)\n",
    "        zero_idx = np.where(np.isinf(aligned_feature))\n",
    "        aligned_feature[zero_idx] = 0\n",
    "\n",
    "        pbar_small.close()\n",
    "        return aligned_sub,aligned_feature\n",
    "    \n",
    "    def align_upsampling_and_save(self, reference, id_idx, collapse_function=None, epsilon = 10e-6):\n",
    "        folder = '/data/mifs_scratch/yw454/cmumosei_aligned'\n",
    "        log_file = './mosei_alignment_log.txt'\n",
    "        #aligned_output = {}\n",
    "        count = 0\n",
    "        \n",
    "        ##self.computational_sequences.keys are COVERAP, OpenFace, WordVec, etc\n",
    "        #for sequence_name in self.computational_sequences.keys():\n",
    "        #    #init a dictionary to store different featues seperately\n",
    "        #    aligned_output[sequence_name]={}\n",
    "        \n",
    "        if reference not in self.computational_sequences.keys():\n",
    "            log.error(\"Computational sequence <%s> does not exist in dataset\"%reference,error=True)\n",
    "        \n",
    "        #get data of reference feature\n",
    "        refseq=self.computational_sequences[reference].data\n",
    "        #unifying the dataset, removing any entries that are not in the reference computational sequence\n",
    "        self.unify()\n",
    "        \n",
    "        #building the relevant entries to the reference - what we do in this section is simply removing all the [] from the entry ids and populating them into a new dictionary\n",
    "        log.status(\"Pre-alignment based on <%s> computational sequence started ...\"%reference)\n",
    "        \n",
    "        relevant_entries=self.get_relevant_entries(reference)\n",
    "        log.status(\"Alignment starting ...\")\n",
    "        \n",
    "        \n",
    "        pbar = log.progress_bar(total=len(refseq.keys()),unit=\" Computational Sequence Entries\",leave=False)\n",
    "        pbar.set_description(\"Overall Progress\")\n",
    "        # for some_id in all video ids\n",
    "        for entry_key in list(refseq.keys()):\n",
    "            \n",
    "            if entry_key in id_idx:\n",
    "                stored_idx = id_idx.index(entry_key)\n",
    "                #if stored_idx < 104 or (stored_idx > 104 and stored_idx < 1781):\n",
    "                if stored_idx < 1781 or stored_idx == 1815:\n",
    "                    continue\n",
    "            \n",
    "            all_intersects = {}\n",
    "            all_intersect_features = {}\n",
    "            \n",
    "            #for sequence_name in self.computational_sequences.keys():\n",
    "            #    all_intersects[sequence_name] = []\n",
    "            #    all_intersect_features[sequence_name] = []\n",
    "            \n",
    "            ref_all=refseq[entry_key]['intervals']\n",
    "                \n",
    "            #aligning all sequences to ref sequence (previous: align refer to refer as well, now: not include refer)\n",
    "            #otherseq_key: other features; entry_key: some video id\n",
    "                \n",
    "            for otherseq_key in list(self.computational_sequences.keys()):\n",
    "                if otherseq_key != reference:\n",
    "                    feature_info = 'reference: {}, other feature {}, video id: {}'.format(reference, otherseq_key, entry_key)\n",
    "                    intersects,intersects_features=self.intersect_and_copy_upsampling(ref_all,relevant_entries[otherseq_key][entry_key],epsilon, log_file, feature_info)\n",
    "                else:\n",
    "                    intersects,intersects_features=refseq[entry_key]['intervals'][:,:],refseq[entry_key]['features'][:,:]\n",
    "                    \n",
    "                #print(type(intersects[0]))\n",
    "                #print(type(intersects_features[0]))\n",
    "                #print(len(intersects[0]))\n",
    "                #print(len(intersects_features[0]))\n",
    "                all_intersects[otherseq_key] = intersects\n",
    "                all_intersect_features[otherseq_key] = intersects_features\n",
    "                    \n",
    "\n",
    "            #save features per video\n",
    "            for sequence_name in self.computational_sequences.keys():\n",
    "                video_code = id_idx.index(entry_key)\n",
    "                video_code = str(video_code).zfill(6)\n",
    "                \n",
    "                save_htk_format(all_intersect_features[sequence_name], sequence_name, folder, video_code)\n",
    "                save_intervals(all_intersects[sequence_name], sequence_name, folder, video_code)\n",
    "                print('alignment saved for video {}.'.format(video_code))\n",
    "            \n",
    "            pbar.update(1)\n",
    "        pbar.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define your different modalities - refer to the filenames of the CSD files\n",
    "basic_dict={'COVAREP': DATA_PATH + 'CMU_MOSEI_COVAREP.csd', \n",
    "            'WordVec': DATA_PATH + 'CMU_MOSEI_TimestampedWordVectors.csd'}\n",
    "second_dict = {'Facet': DATA_PATH + 'CMU_MOSEI_VisualFacet42.csd',\n",
    "            'OpenFace': DATA_PATH + 'CMU_MOSEI_VisualOpenFace2.csd'}\n",
    "other_dict = {'Word': DATA_PATH + 'CMU_MOSEI_TimestampedWords.csd',\n",
    "             'Phone': DATA_PATH + 'CMU_MOSEI_TimestampedPhones.csd'}\n",
    "label_dict = {'mylabels':DATA_PATH + 'CMU_MOSEI_Labels.csd'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMU_MOSEI_VisualOpenFace2.csd\n",
      "CMU_MOSEI_TimestampedWords.csd\n",
      "CMU_MOSEI_COVAREP.csd\n",
      "CMU_MOSEI_VisualFacet42.csd\n",
      "CMU_MOSEI_TimestampedWordVectors.csd\n",
      "CMU_MOSEI_TimestampedPhones.csd\n",
      "CMU_MOSEI_Labels.csd\n",
      "COVAREP.zip\n"
     ]
    }
   ],
   "source": [
    "# list the directory contents... let's see what features there are\n",
    "data_files = os.listdir(DATA_PATH)\n",
    "print('\\n'.join(data_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validators eliminated\n",
      "\u001b[92m\u001b[1m[2021-01-24 02:53:43.772] | Success | \u001b[0mComputational sequence read from file /data/mifs_scratch/yw454/cmu_mosei_latest/CMU_MOSEI_COVAREP.csd ...\n",
      "\u001b[93m\u001b[1m[2021-01-24 02:53:43.945] | Warning | \u001b[0mValidation of the computational sequence skipped by user request.\n",
      "validators eliminated\n",
      "\u001b[92m\u001b[1m[2021-01-24 02:53:43.945] | Success | \u001b[0mComputational sequence read from file /data/mifs_scratch/yw454/cmu_mosei_latest/CMU_MOSEI_TimestampedWordVectors.csd ...\n",
      "\u001b[93m\u001b[1m[2021-01-24 02:53:44.099] | Warning | \u001b[0mValidation of the computational sequence skipped by user request.\n",
      "\u001b[92m\u001b[1m[2021-01-24 02:53:44.099] | Success | \u001b[0mDataset initialized successfully ... \n",
      "validators eliminated\n",
      "\u001b[92m\u001b[1m[2021-01-24 02:53:44.100] | Success | \u001b[0mComputational sequence read from file /data/mifs_scratch/yw454/cmu_mosei_latest/CMU_MOSEI_VisualFacet42.csd ...\n",
      "\u001b[93m\u001b[1m[2021-01-24 02:53:44.650] | Warning | \u001b[0mValidation of the computational sequence skipped by user request.\n",
      "validators eliminated\n",
      "\u001b[92m\u001b[1m[2021-01-24 02:53:44.651] | Success | \u001b[0mComputational sequence read from file /data/mifs_scratch/yw454/cmu_mosei_latest/CMU_MOSEI_VisualOpenFace2.csd ...\n",
      "\u001b[93m\u001b[1m[2021-01-24 02:53:44.797] | Warning | \u001b[0mValidation of the computational sequence skipped by user request.\n",
      "\u001b[92m\u001b[1m[2021-01-24 02:53:44.797] | Success | \u001b[0mDataset initialized successfully ... \n",
      "validators eliminated\n",
      "\u001b[92m\u001b[1m[2021-01-24 02:53:44.897] | Success | \u001b[0mComputational sequence read from file /data/mifs_scratch/yw454/cmu_mosei_latest/CMU_MOSEI_Labels.csd ...\n",
      "\u001b[93m\u001b[1m[2021-01-24 02:53:45.069] | Warning | \u001b[0mValidation of the computational sequence skipped by user request.\n",
      "\u001b[92m\u001b[1m[2021-01-24 02:53:45.070] | Success | \u001b[0mDataset initialized successfully ... \n"
     ]
    }
   ],
   "source": [
    "basic_dataset = new_mmdataset(basic_dict)\n",
    "second_dataset = new_mmdataset(second_dict)\n",
    "label_dataset = new_mmdataset(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "[COVAREP, WordVec] = [basic_dataset.computational_sequences['COVAREP'],\n",
    "                        basic_dataset.computational_sequences['WordVec']]\n",
    "label = label_dataset.computational_sequences['mylabels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dawna/ql264/software/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1403\n",
      "295\n",
      "293\n",
      "544\n"
     ]
    }
   ],
   "source": [
    "not_enough_word = set()\n",
    "not_enough_label = set()\n",
    "no_labels = set()\n",
    "label_keys = list(label.keys())\n",
    "for k in list(COVAREP.keys()):\n",
    "    acoustic = COVAREP[k]['intervals'][-1][1]\n",
    "    language = WordVec[k]['intervals'][-1][1]\n",
    "    \n",
    "    if language / acoustic > 2 or acoustic / language > 2:\n",
    "        not_enough_word.add(k)\n",
    "    \n",
    "    if k in label_keys:\n",
    "        annotations = label[k]['intervals'][-1][1] - label[k]['intervals'][0][0]\n",
    "    else:\n",
    "        no_labels.add(k)\n",
    "    \n",
    "    if annotations / acoustic > 2 or acoustic / annotations > 2:\n",
    "        not_enough_label.add(k)\n",
    "    \n",
    "print(len(not_enough_label))\n",
    "print(len(not_enough_word))\n",
    "print(len((not_enough_label & not_enough_word)))\n",
    "print(len(no_labels))   \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m\u001b[1m[2021-01-24 02:53:58.267] | Status  | \u001b[0mUnify was called ...\n",
      "\u001b[93m\u001b[1m[2021-01-24 02:53:58.277] | Warning | \u001b[0mPEBwwe0PLZ8 entry is not shared among all sequences, removing it ...\n",
      "\u001b[92m\u001b[1m[2021-01-24 02:53:58.280] | Success | \u001b[0mUnify completed ...\n",
      "\u001b[94m\u001b[1m[2021-01-24 02:53:58.280] | Status  | \u001b[0mPre-alignment based on <COVAREP> computational sequence started ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/3836 [00:00<?, ? Computational Sequence Entries/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Overall Progress:   0%|          | 0/3836 [00:00<?, ? Computational Sequence Entries/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/5459 [00:00<?, ? Segments/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Aligning: reference: COVAREP, other feature WordVec, video id: HUOMbK1x7MI:   0%|          | 0/5459 [00:00<?, ? Segments/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m\u001b[1m[2021-01-24 02:54:19.297] | Status  | \u001b[0mPre-alignment done for <WordVec> ...\n",
      "\u001b[94m\u001b[1m[2021-01-24 02:54:20.331] | Status  | \u001b[0mAlignment starting ...\n",
      "(5459, 2)\n",
      "(5459, 74)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Aligning: reference: COVAREP, other feature WordVec, video id: HUOMbK1x7MI:   9%|▉         | 513/5459 [00:00<00:00, 5128.32 Segments/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Aligning: reference: COVAREP, other feature WordVec, video id: HUOMbK1x7MI:  22%|██▏       | 1212/5459 [00:00<00:00, 5572.12 Segments/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Aligning: reference: COVAREP, other feature WordVec, video id: HUOMbK1x7MI:  35%|███▌      | 1926/5459 [00:00<00:00, 5965.00 Segments/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Aligning: reference: COVAREP, other feature WordVec, video id: HUOMbK1x7MI:  48%|████▊     | 2647/5459 [00:00<00:00, 6289.07 Segments/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Aligning: reference: COVAREP, other feature WordVec, video id: HUOMbK1x7MI:  62%|██████▏   | 3361/5459 [00:00<00:00, 6520.21 Segments/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Aligning: reference: COVAREP, other feature WordVec, video id: HUOMbK1x7MI:  75%|███████▍  | 4084/5459 [00:00<00:00, 6717.07 Segments/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Aligning: reference: COVAREP, other feature WordVec, video id: HUOMbK1x7MI:  88%|████████▊ | 4807/5459 [00:00<00:00, 6861.71 Segments/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COVAREP\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(74,)\n",
      "alignment saved for video 001815.\n",
      "WordVec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(300,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Overall Progress:   0%|          | 1/3836 [00:06<6:51:32,  6.44s/ Computational Sequence Entries]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alignment saved for video 001815.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "video_id_record = os.path.join('/data/mifs_scratch/yw454/cmumosei_aligned', 'video_id.json')\n",
    "\n",
    "with open(video_id_record, 'r') as json_file:\n",
    "    video_ids = json.load(json_file)\n",
    "\n",
    "basic_dataset.align_upsampling_and_save('COVAREP', video_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5459, 74)\n"
     ]
    }
   ],
   "source": [
    "print(COVAREP['HUOMbK1x7MI']['features'].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
